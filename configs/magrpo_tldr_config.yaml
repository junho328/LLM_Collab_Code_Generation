# Configuration for TLDR summarization training with MAGRPO
# Exact parameters from train_tldr.py

# Model configuration
model:
  name: "Qwen/Qwen3-1.7B"  # Exact model from train_tldr.py
  type: "qwen"
  temperature: 0.7
  top_p: 0.9
  max_length: 2048
  tokenizer_kwargs:
    trust_remote_code: true
  model_kwargs:
    trust_remote_code: true
    torch_dtype: "auto"

# Dataset configuration
dataset:
  name: "trl-lib/tldr"
  type: "tldr"  # Used to select formatters and reward function
  train_split: "train[:1000]"  # Exact value from train_tldr.py
  eval_split: "test[:1000]"   # Exact value from train_tldr.py

# Tokenizer configuration
tokenizer:
  padding_side: "left"  # Specific to tldr setup

# Output configuration
output:
  base_dir: "./magrpo_output"
  save_final_model: true
  save_path: "./magrpo_output/tldr"  # Exact save path from train_tldr.py

# MAGRPO training configuration
magrpo:
  num_train_epochs: 1  # Exact value from train_tldr.py
  per_device_train_batch_size: 1
  learning_rate: 5.0e-6  # Exact value from train_tldr.py
  logging_steps: 10  # Exact value from train_tldr.py
  save_steps: 100  # Exact value from train_tldr.py
  num_generations: 4
  max_new_tokens: 256
  # Note: temperature and top_p not specified in original, will use model defaults

# Multi-agent configuration
agents:
  num_agents: 2

# Reward processor configuration
reward_processor:
  enabled: true
  scale_factor: 1  # RewardProcessors.scale(factor=1) from train_tldr.py

# Wandb configuration
wandb:
  project: "mlrl"
  entity: "nu-llpr"
  name: "magrpo_tldr"  # Will be appended with model name in script
  tags: ["magrpo", "tldr", "multi-agent"]