# model
model:
  name: "/home/work/aipr-jhna/huggingface_hub/Qwen2.5-Coder-1.5B"
  type: "qwen"
  temperature: 0.7
  top_p: 0.9
  max_length: 2048
  tokenizer_kwargs:
    trust_remote_code: true
  model_kwargs:
    trust_remote_code: true
    torch_dtype: "bfloat16"
  special_tokens: {}

# LoRA PEFT configuration
lora:
  enabled: false
  r: 32  # rank of the update matrices
  lora_alpha: 32  # scaling factor
  lora_dropout: 0.0
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  bias: "none"
  task_type: "CAUSAL_LM"

# dataset
dataset:
  name: "/home/work/aipr-jhna/huggingface_hub/openai_humaneval"
  type: "humaneval"
  train_split: "test[33:163]"
  eval_split: "test[:32]"

seed: 42

# output
output:
  base_dir: "/home/work/aipr-jhna/output"
  save_final_model: false
  verbose: false

# external
external:
  mode: "level_feedback"
  sandbox_slice: 1

# magrpo
magrpo:
  num_turns: 1
  num_train_epochs: 3
  per_device_train_batch_size: 1
  rollout_buffer_size: 4
  learning_rate: 2.0e-5
  eval_interval: 4
  eval_num_samples: 32
  num_generations: 4
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  top_k: null
  joint_mode: aligned
  num_agents: 2
  discount: 0.9
  termination_threshold: -0.2

  logging_steps: 5
  save_steps: 200

reward_processor:
  enabled: true
  scale_factor: 1.0
  shift: -4

# Collaboration enforcement settings
# Prevents main function from defining its own aux function
collaboration:
  enforce: true  # If true, penalize main defining its own aux function
  self_aux_penalty: 0.0  # Reward value when main defines its own aux (strong penalty)

# Agent Chaining configuration
# When enabled, Agent 2 first reasons about Agent 1's aux function, then generates main
# Agent 2's single output includes: <predicted_aux>...</predicted_aux> + main function
agent_chaining:
  enabled: true  # Set to true to enable agent chaining mode
  max_new_tokens_per_agent: [256, 384]  # [Agent 1 (aux), Agent 2 (reasoning + main)]
  format_reward_weight: 0.5  # Weight for format reward (how well predicted aux matches actual)

# Chat Template configuration (for Instruct models)
# When enabled, applies chat template to prompts and extracts assistant response from completions
chat_template:
  enabled: true  # Set to true for Instruct models (e.g., Qwen2.5-Coder-1.5B-Instruct)
  system_prompt: "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."

# wandb
wandb:
  project: "HumanEval"
  entity: "contrl"
  name: "acgrpo_he_1.5b-instruct-shift-4-coop_penalty0.0-must_call_aux-full"
  dir: "output"
  tags: ["magrpo", "humaneval", "multi-agent"]
