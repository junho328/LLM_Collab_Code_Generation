# models (heterogeneous agents - each agent uses a different model)
models:
  # Agent 0: Auxiliary function generator
  - name: "/home/work/aipr-jhna/huggingface_hub/starcoder2-3b"
    type: "gpt"
    temperature: 0.7
    top_p: 0.9
    max_length: 2048
    tokenizer_kwargs:
      trust_remote_code: true
    model_kwargs:
      trust_remote_code: true
      torch_dtype: "bfloat16"
    special_tokens: {}
  - name: "/home/work/aipr-jhna/huggingface_hub/Qwen2.5-Coder-1.5B"
    type: "qwen"
    temperature: 0.7
    top_p: 0.9
    max_length: 2048
    tokenizer_kwargs:
      trust_remote_code: true
    model_kwargs:
      trust_remote_code: true
      torch_dtype: "bfloat16"
    special_tokens: {}
  # Agent 1: Main function generator
  # - name: "bigcode/starcoder2-3b"
  #   type: "gpt"
  #   temperature: 0.7
  #   top_p: 0.9
  #   max_length: 2048
  #   tokenizer_kwargs:
  #     trust_remote_code: true
  #   model_kwargs:
  #     trust_remote_code: true
  #     torch_dtype: "bfloat16"
  #   special_tokens: {}

# dataset
dataset:
  name: "/home/work/aipr-jhna/huggingface_hub/openai_humaneval" #"openai/openai_humaneval"
  type: "humaneval"
  train_split: "test[33:163]"
  eval_split: "test[:32]"

seed: 42

# output
output:
  base_dir: "/home/work/aipr-jhna/output/comlrl_output/agent1-star_agent2-qwen/"
  save_final_model: false
  verbose: false

# external
external:
  mode: "level_feedback"
  sandbox_slice: 1

# magrpo
magrpo:
  num_turns: 1
  num_train_epochs: 4
  per_device_train_batch_size: 1
  rollout_buffer_size: 512
  learning_rate: 2.0e-5
  eval_interval: 4
  eval_num_samples: 4
  num_generations: 4
  max_new_tokens: 256
  temperature: 0.6
  top_p: 0.6
  top_k: null
  joint_mode: aligned
  num_agents: 2
  discount: 0.9
  termination_threshold: -0.2

  logging_steps: 1
  save_steps: 200

reward_processor:
  enabled: true
  scale_factor: 1.0
  shift: -4
# wandb
wandb:
  project: "MAGRPO-Code-different"
  entity: "contrl"
  name: "magrpo_humaneval_agent1-star_agent2-qwen"
  dir: "output"
  tags: ["magrpo", "humaneval", "multi-agent"]
